{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **supervised** in _supervided learning_ refers to the fact that each sample within the data being used to build the system contains an associated label.<br>\n",
    "The goal is to build a model that can accurately predict the value on the label when presented with new data.That is, the goal is to approximate the mapping function so well that when you have new input data (x) that you can predict the output variables (Y) for that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Y = f(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be further grouped into **regression** and **classification** problems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Regression**: A regression problem is when the output variavle is a real value, such as \"dollars\";\n",
    "- **Classification** : A classification problem is when the output variable is a category such as \"yes\" or \"no\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some popular examples of supervised machine learning algorithms are:\n",
    "- Linear regression for regression problems;\n",
    "- Random forest for classification and regression problems;\n",
    "- Support vector machines for classidication problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is simple, which makes it a good place to start thinking about algorithms more generally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y' = A * X + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- y-hat is the output, or guess made by the algorithm, the dependent variable;\n",
    "- A is the coefficient. it's also the slope of the line that expresses the relationship between X and Y-hat;\n",
    "- X is the input, the given or independent variable;\n",
    "- B is the intercept, where the line crosses the Y axis;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression** expresses a linear relationship between the input x and the output y. For every change in X, Y-hat will change by the same amount no matter how far along the line you are. The x is transformed by the same A and B at every point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we're dealing with Y-hat, an estimate about the real value of y, is because linear regression is a formula used to estimate real values, the error is inevitable. Linear regression is often used to \"fit\" a scatter plot of given x-y pairs. A good fit minimizes the error between y-hat and the actual y. That is, choosing the right A and B will minimize the sume of the differences between each Y and its respective Y-hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is kind of like linear regression but is used when the dependent variable is not a number, but something else (like Yes/No). Its called regression but performs classification as based on the regression it classifies the dependent variable into either of the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L.R. is used for predicition of output which is binary, as stated above.\n",
    "\n",
    "y = b_0 + b_1 * x (Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, logistic regression takes input data and classifies it as category or not category, on or off expressed as 1 or 0, based on the strength of the input's signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes input dara and squishes it, so that no matter what the range of the input is, it will be compressed into the space between 1 and 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a series of nodes, a direction graph that starts at the base with a single node and extends to the many leaf nodes that represent the categories that the tree can classify. Another way to think of a decision tree is as a flow chart, where the flow starts at the root node and ends with a decision made at the leaves. It is a decision-support tool. It uses a tree-like graph to show the predictions that result from a series of featyre-based splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful terms for describing a decision tree:\n",
    "- **Root Node**: A root node is at the beginning of a       tree. It represents entire population being analyzed.     From the root node, the population is divided according   to various features, and those sub-groups are split in   turn at each desicion node under the root node.\n",
    "- **Slitting**: It is a process of dividing a note into     two or more sub-nodes.\n",
    "- **Decision Node**: When a sub-node splits into further   sub-nodes, it's a decision node.\n",
    "- **Leaf Node** or **Terminal Node** : Nodes taht do ot     split are called leaf or terminal nodes.\n",
    "- **Pruning**: Removing the sub-nodes of a parent node is   called pruning. A tree is glown through splitting and     shrunk through pruning.\n",
    "- **Branch or Sub-Tree**: A sub-section of decision tree   is called branch or a sub-tree, just as a portion of a   graph is called a sub-graph.\n",
    "- **Parent Node and Child Node**: These are relative       terms. Any node that falls under another node is a       child node or sub-node, and any node which precedes       those child nodes is called a parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are a popular algorithm for several reasons:\n",
    "\n",
    "**Vantages**\n",
    "\n",
    "- **Explanatory Power** : The output of decision trees is    interpretable. It can be understood by people without    analytical or mathematical backgrounds. It does not     require any statistical knowledge to interpret them.\n",
    "- **Exploratory data analysis** : Decision trees can       enable analysts to identify significant variables and     important relations between two or more variables,       helping to surface the signal contained by many input     variables.\n",
    "- **Minimal data cleaning** : Because decision trees are   resilient to outliers and missing values, they require   less data cleaning than some other algorithms.\n",
    "- **Any data type** : Decision trees can make               classifications based on both numerical and categorical   variables.\n",
    "- **Non-parametric** : A decision tree is a non-           parametric algorithm, as opposed to neural networks,     which process input data transformed into a tensor, via   tensor multiplication using large number of               coefficients, known as parameters.\n",
    "\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "- **Overfitting** : Over fitting is a common flaw of       decision trees. Setting constraints on model parameters   and making the model simpler through pruning are two     ways to regularize a decision tree.\n",
    "- **Predicting continuous variables** : While decision     trees can ingest continuous numerical input, they are     not a practical way to predict such values, since         decision-tree predictions must be separated into         discrete categories, which results in a loss of           information when applying the model to continuous         values.\n",
    "- **Heavy feature engineering**: The flip side of a         decision tree’s explanatory power is that it requires     heavy feature engineering. When dealing with             unstructured data or data with latent factors, this       makes decision trees sub-optimal. Neural networks are     clearly superior in this regard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are made of many decision trees. They are ensembles of decision trees, each decision tree created by using a subset of the attributes used to classify a given population (they are sub-trees, see above). Those decision trees vote on how to classify a given instance of input data, and the random forest bootstraps those votes to choose the best prediction. This is done to prevent overfitting, a common flaw of decision trees.\n",
    "\n",
    "A random forest is a supervised classification algorithm. It creates a forest (many decision trees) and orders their nodes and splits randomly. The more trees in the forest, the better the results it can produce.\n",
    "\n",
    "If you input a training dataset with targets and features into the decision tree, it will formulate some set of rules that can be used to perform predictions.\n",
    "\n",
    "Example: You want to predict whether a visitor to your e-commerce Web site will enjoy a mystery novel. First, collect information about past books they’ve read and liked. Metadata about the novels will be the input; e.g. number of pages, author, publication date, which series it’s part of if any. The decision tree contains rules that apply to those features; for example, some readers like very long books and some don’t. Inputting metadata about new novels will result in a prediction regarding whether or not the Web site visitor in question would like that novel. Arranging the nodes and defining the rules relies on information gain and Gini-index calculations. With random forests, finding the root node and splitting the feature nodes is done randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
